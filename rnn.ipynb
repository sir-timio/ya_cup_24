{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_FOLDER = \"./YandexCup2024v2\"\n",
    "\n",
    "TRAIN_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTrain\")\n",
    "TEST_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ids of a dataset\n",
    "\n",
    "def read_testcase_ids(dataset_path: str):\n",
    "    ids = [int(case_id) for case_id in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, case_id))]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = read_testcase_ids(TRAIN_DATASET_PATH)\n",
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = read_testcase_ids(TEST_DATASET_PATH)\n",
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFilePaths:\n",
    "    def __init__(self, testcase_path: str):\n",
    "        self.testcase_path = testcase_path\n",
    "        \n",
    "    def localization(self):\n",
    "        return os.path.join(self.testcase_path, 'localization.csv')\n",
    "    \n",
    "    def control(self):\n",
    "        return os.path.join(self.testcase_path, 'control.csv')\n",
    "    \n",
    "    def metadata(self):\n",
    "        return os.path.join(self.testcase_path, 'metadata.json')\n",
    "    \n",
    "    # exists only for test_dataset\n",
    "    def requested_stamps(self):\n",
    "        return os.path.join(self.testcase_path, 'requested_stamps.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_localization(localization_path: str):\n",
    "    return pd.read_csv(localization_path)\n",
    "\n",
    "def read_control(control_path):\n",
    "    return pd.read_csv(control_path)\n",
    "\n",
    "def read_metadata(metadata_path: str):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_requested_stamps(requested_stamps_path: str):\n",
    "    return pd.read_csv(requested_stamps_path)\n",
    "    \n",
    "def read_testcase(dataset_path: str, testcase_id: str, is_test: bool = False):\n",
    "    testcase_path = os.path.join(dataset_path, str(testcase_id))\n",
    "    data_file_paths = DataFilePaths(testcase_path)\n",
    "    \n",
    "    testcase_data = {}\n",
    "    testcase_data['localization'] = read_localization(data_file_paths.localization())\n",
    "    testcase_data['control'] = read_control(data_file_paths.control())\n",
    "    testcase_data['metadata'] = read_metadata(data_file_paths.metadata())\n",
    "    if is_test:\n",
    "        testcase_data['requested_stamps'] = read_requested_stamps(data_file_paths.requested_stamps())\n",
    "        \n",
    "    return testcase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testcases(dataset_path: str, is_test: bool = False, testcase_ids: typing.Iterable[int] = None):\n",
    "    result = {}\n",
    "    if testcase_ids is None:\n",
    "        testcase_ids = read_testcase_ids(dataset_path)\n",
    "    \n",
    "    for testcase_id in tqdm(testcase_ids):\n",
    "        testcase = read_testcase(dataset_path, testcase_id, is_test=is_test)\n",
    "        result[testcase_id] = testcase\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 226.08it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = read_testcases(TRAIN_DATASET_PATH,  testcase_ids = train_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['localization', 'control', 'metadata'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1582, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['localization'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>acceleration_level</th>\n",
       "      <th>steering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987440736</td>\n",
       "      <td>-114</td>\n",
       "      <td>-2.655140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3027341070</td>\n",
       "      <td>-123</td>\n",
       "      <td>-2.598169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3066793076</td>\n",
       "      <td>-132</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3106757146</td>\n",
       "      <td>-141</td>\n",
       "      <td>-2.544422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3146784622</td>\n",
       "      <td>-147</td>\n",
       "      <td>-2.488557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>62786741116</td>\n",
       "      <td>33</td>\n",
       "      <td>117.135357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>62826899778</td>\n",
       "      <td>33</td>\n",
       "      <td>119.059706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>62867315073</td>\n",
       "      <td>33</td>\n",
       "      <td>120.952111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>62906605994</td>\n",
       "      <td>32</td>\n",
       "      <td>122.802597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>62946753648</td>\n",
       "      <td>32</td>\n",
       "      <td>124.656518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stamp_ns  acceleration_level    steering\n",
       "0      2987440736                -114   -2.655140\n",
       "1      3027341070                -123   -2.598169\n",
       "2      3066793076                -132   -2.544422\n",
       "3      3106757146                -141   -2.544422\n",
       "4      3146784622                -147   -2.488557\n",
       "...           ...                 ...         ...\n",
       "1495  62786741116                  33  117.135357\n",
       "1496  62826899778                  33  119.059706\n",
       "1497  62867315073                  33  120.952111\n",
       "1498  62906605994                  32  122.802597\n",
       "1499  62946753648                  32  124.656518\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42000/42000 [03:55<00:00, 178.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# may take some time\n",
    "train_dataset = read_testcases(TRAIN_DATASET_PATH,  testcase_ids = train_ids[:])\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:25<00:00, 314.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = read_testcases(TEST_DATASET_PATH, is_test=True)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all possible categories from the training data\n",
    "vehicle_models = []\n",
    "vehicle_modifications = []\n",
    "tire_types = []\n",
    "\n",
    "for testcase in train_dataset.values():\n",
    "    metadata = testcase['metadata']\n",
    "    vehicle_models.append(metadata['vehicle_model'])\n",
    "    vehicle_modifications.append(metadata['vehicle_model_modification'])\n",
    "    tires = metadata['tires']\n",
    "    tire_types.append(tires['front'])\n",
    "    tire_types.append(tires['rear'])\n",
    "\n",
    "# Fit the encoders\n",
    "vehicle_model_encoder.fit(vehicle_models)\n",
    "vehicle_modification_encoder.fit(vehicle_modifications)\n",
    "tires_encoder.fit(tire_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 60036),\n",
       " (2, 7937),\n",
       " (5, 4870),\n",
       " (6, 3453),\n",
       " (3, 2518),\n",
       " (1, 2373),\n",
       " (7, 1909),\n",
       " (4, 492),\n",
       " (11, 170),\n",
       " (9, 80),\n",
       " (12, 78),\n",
       " (8, 54),\n",
       " (13, 16),\n",
       " (10, 14)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(tire_types).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataset, normalizer=None, training=True):\n",
    "        self.data = []\n",
    "        self.training = training\n",
    "        sampling_interval_ns = 4e7  # 0.04 seconds in nanoseconds\n",
    "        initial_state_length = int(5 / 0.04)  # Steps for initial 5 seconds (125 steps)\n",
    "        target_length = int(15 / 0.04)  # Steps from 5s to 20s (375 steps)\n",
    "        sequence_length = initial_state_length + target_length  # Total steps (500 steps)\n",
    "\n",
    "        # For normalization\n",
    "        self.positions = []\n",
    "        self.controls = []\n",
    "\n",
    "        for testcase_id, testcase in dataset.items():\n",
    "            # Get metadata\n",
    "            metadata = testcase['metadata']\n",
    "            vehicle_model = vehicle_model_encoder.transform([metadata['vehicle_model']])[0]\n",
    "            vehicle_modification = vehicle_modification_encoder.transform([metadata['vehicle_model_modification']])[0]\n",
    "            tires_front = tires_encoder.transform([metadata['tires']['front']])[0]\n",
    "            tires_rear = tires_encoder.transform([metadata['tires']['rear']])[0]\n",
    "            vehicle_features = [vehicle_model, vehicle_modification, tires_front, tires_rear]\n",
    "\n",
    "            # Get control commands\n",
    "            control = testcase['control']\n",
    "            control['acceleration_level'] = control['acceleration_level'].fillna(0)\n",
    "            control_seq = control[['stamp_ns', 'acceleration_level', 'steering']].values\n",
    "\n",
    "            # Get localization data\n",
    "            localization = testcase['localization']\n",
    "            localization_seq = localization[['stamp_ns', 'x', 'y', 'yaw']].values\n",
    "\n",
    "            # Resample to fixed time steps (every 0.04s)\n",
    "            time_steps = np.arange(0, 60 * 1e9, sampling_interval_ns)\n",
    "            control_resampled = self.resample_sequence(control_seq, time_steps)\n",
    "            localization_resampled = self.resample_sequence(localization_seq, time_steps)\n",
    "\n",
    "            max_start_idx = len(time_steps) - sequence_length\n",
    "            for i in range(0, max_start_idx, initial_state_length):  # Slide window\n",
    "                # Initial localization sequence (first 5 seconds)\n",
    "                initial_localization = localization_resampled[i:i+initial_state_length, 1:]  # Shape: [125, 3]\n",
    "                # Convert yaw to sin and cos\n",
    "                yaw = initial_localization[:, 2]\n",
    "                sin_yaw = np.sin(yaw)\n",
    "                cos_yaw = np.cos(yaw)\n",
    "                initial_localization = np.hstack((initial_localization[:, :2], sin_yaw[:, np.newaxis], cos_yaw[:, np.newaxis]))  # Shape: [125, 4]\n",
    "\n",
    "                # Control sequence from t + 5s to t + 20s (for target trajectory)\n",
    "                control_sequence = control_resampled[i+initial_state_length:i+sequence_length, 1:]  # Shape: [375, 2]\n",
    "\n",
    "                # Target trajectory from t + 5s to t + 20s\n",
    "                target_traj = localization_resampled[i+initial_state_length:i+sequence_length, 1:]  # Shape: [375, 3]\n",
    "                # Convert yaw to sin and cos\n",
    "                yaw = target_traj[:, 2]\n",
    "                sin_yaw = np.sin(yaw)\n",
    "                cos_yaw = np.cos(yaw)\n",
    "                target_traj = np.hstack((target_traj[:, :2], sin_yaw[:, np.newaxis], cos_yaw[:, np.newaxis]))  # Shape: [375, 4]\n",
    "\n",
    "                # Collect data for normalization\n",
    "                if training:\n",
    "                    self.positions.append(initial_localization[:, :2])\n",
    "                    self.positions.append(target_traj[:, :2])\n",
    "                    self.controls.append(control_sequence)\n",
    "\n",
    "                self.data.append({\n",
    "                    'vehicle_features': vehicle_features,\n",
    "                    'initial_localization': initial_localization,\n",
    "                    'control_sequence': control_sequence,\n",
    "                    'target_traj': target_traj\n",
    "                })\n",
    "\n",
    "        # Fit normalizer if training\n",
    "        if training:\n",
    "            self.positions = np.vstack(self.positions)\n",
    "            self.controls = np.vstack(self.controls)\n",
    "            self.normalizer = {\n",
    "                'x_mean': self.positions[:, 0].mean(),\n",
    "                'x_std': self.positions[:, 0].std(),\n",
    "                'y_mean': self.positions[:, 1].mean(),\n",
    "                'y_std': self.positions[:, 1].std(),\n",
    "                'acceleration_mean': self.controls[:, 0].mean(),\n",
    "                'acceleration_std': self.controls[:, 0].std(),\n",
    "                'steering_mean': self.controls[:, 1].mean(),\n",
    "                'steering_std': self.controls[:, 1].std(),\n",
    "            }\n",
    "        else:\n",
    "            self.normalizer = normalizer\n",
    "\n",
    "    def resample_sequence(self, seq, time_steps):\n",
    "        df_seq = pd.DataFrame(seq, columns=['stamp_ns'] + [f'feat_{i}' for i in range(seq.shape[1]-1)])\n",
    "        df_seq = df_seq.set_index('stamp_ns').reindex(time_steps, method='nearest').reset_index()\n",
    "        return df_seq.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        vehicle_features = torch.tensor(sample['vehicle_features'], dtype=torch.long)\n",
    "\n",
    "        # Normalize initial localization\n",
    "        initial_localization = sample['initial_localization']\n",
    "        initial_localization[:, 0] = (initial_localization[:, 0] - self.normalizer['x_mean']) / self.normalizer['x_std']\n",
    "        initial_localization[:, 1] = (initial_localization[:, 1] - self.normalizer['y_mean']) / self.normalizer['y_std']\n",
    "        initial_localization = torch.tensor(initial_localization, dtype=torch.float32)\n",
    "\n",
    "        # Normalize control sequence\n",
    "        control_sequence = sample['control_sequence']\n",
    "        control_sequence[:, 0] = (control_sequence[:, 0] - self.normalizer['acceleration_mean']) / self.normalizer['acceleration_std']\n",
    "        control_sequence[:, 1] = (control_sequence[:, 1] - self.normalizer['steering_mean']) / self.normalizer['steering_std']\n",
    "        control_sequence = torch.tensor(control_sequence, dtype=torch.float32)\n",
    "\n",
    "        # Normalize target trajectory\n",
    "        target_traj = sample['target_traj']\n",
    "        target_traj[:, 0] = (target_traj[:, 0] - self.normalizer['x_mean']) / self.normalizer['x_std']\n",
    "        target_traj[:, 1] = (target_traj[:, 1] - self.normalizer['y_mean']) / self.normalizer['y_std']\n",
    "        target_traj = torch.tensor(target_traj, dtype=torch.float32)\n",
    "\n",
    "        return vehicle_features, initial_localization, control_sequence, target_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrajectoryDataset(train_dataset, training=True)\n",
    "normalizer = train_data.normalizer\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "# Pass the normalizer to validation dataset\n",
    "val_dataset.dataset.normalizer = normalizer\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle_features, initial_localization, control_sequence, target_traj in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for vehicle_features, initial_localization, control_sequence, target_traj in tqdm(train_loader):\n",
    "#     vehicle_features = vehicle_features.to(device)\n",
    "#     initial_localization = initial_localization.to(device)\n",
    "#     control_sequence = control_sequence.to(device)\n",
    "#     target_traj = target_traj.to(device)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(vehicle_features, initial_localization, control_sequence)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Vehicle feature embedding\n",
    "        self.vehicle_embedding = nn.Embedding(num_embeddings=1000, embedding_dim=16)\n",
    "        self.vehicle_fc = nn.Linear(16, hidden_size)\n",
    "\n",
    "        # Initial localization embedding\n",
    "        self.localization_lstm = nn.LSTM(input_size=4, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Control sequence embedding\n",
    "        self.control_lstm = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = nn.LSTM(input_size=hidden_size * 2, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, 4)  # Output x, y, sin(yaw), cos(yaw)\n",
    "\n",
    "    def forward(self, vehicle_features, initial_localization, control_sequence):\n",
    "        batch_size = control_sequence.size(0)\n",
    "        seq_len = control_sequence.size(1)\n",
    "    \n",
    "        # Vehicle feature embedding\n",
    "        vehicle_embedded = self.vehicle_embedding(vehicle_features).mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        vehicle_embedded = self.vehicle_fc(vehicle_embedded)  # [batch_size, hidden_size]\n",
    "        # print(f\"vehicle_embedded shape: {vehicle_embedded.shape}\")\n",
    "    \n",
    "        # Expand vehicle_embedded\n",
    "        vehicle_embedded_expanded = vehicle_embedded.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_size]\n",
    "        # print(f\"vehicle_embedded_expanded shape: {vehicle_embedded_expanded.shape}\")\n",
    "    \n",
    "        # Initial localization embedding\n",
    "        _, (hidden_loc, cell_loc) = self.localization_lstm(initial_localization)\n",
    "        # print(f\"hidden_loc shape: {hidden_loc.shape}\")\n",
    "    \n",
    "        # Control sequence embedding\n",
    "        control_output, _ = self.control_lstm(control_sequence)\n",
    "        # print(f\"control_output shape: {control_output.shape}\")\n",
    "    \n",
    "        # Concatenate\n",
    "        decoder_input = torch.cat((control_output, vehicle_embedded_expanded), dim=2)\n",
    "        # print(f\"decoder_input shape: {decoder_input.shape}\")\n",
    "    \n",
    "        # Decoder LSTM\n",
    "        decoder_output, _ = self.decoder_lstm(decoder_input, (hidden_loc, cell_loc))\n",
    "        # print(f\"decoder_output shape: {decoder_output.shape}\")\n",
    "    \n",
    "        # Output layer\n",
    "        output = self.fc_out(decoder_output)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "    \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEGMENT_LENGTH = 1.0\n",
    "\n",
    "def calculate_metric_on_batch(output_np, target_np, segment_length=1.0):\n",
    "    \"\"\"\n",
    "    output_np: numpy array of shape [batch_size, seq_len, 4], predicted x, y, sin(yaw), cos(yaw)\n",
    "    target_np: numpy array of same shape, ground truth x, y, sin(yaw), cos(yaw)\n",
    "\n",
    "    Returns:\n",
    "        metric: float, the average metric over the batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert sin and cos back to yaw\n",
    "    yaw_pred = np.arctan2(output_np[..., 2], output_np[..., 3])\n",
    "    yaw_gt = np.arctan2(target_np[..., 2], target_np[..., 3])\n",
    "\n",
    "    # Unpack x, y for predictions and targets\n",
    "    x_pred, y_pred = output_np[..., 0], output_np[..., 1]\n",
    "    x_gt, y_gt = target_np[..., 0], target_np[..., 1]\n",
    "\n",
    "    # Compute c1 and c2 for predicted\n",
    "    c1_pred = np.stack([x_pred, y_pred], axis=-1)\n",
    "    c2_pred = c1_pred + segment_length * np.stack([np.cos(yaw_pred), np.sin(yaw_pred)], axis=-1)\n",
    "\n",
    "    # Compute c1 and c2 for ground truth\n",
    "    c1_gt = np.stack([x_gt, y_gt], axis=-1)\n",
    "    c2_gt = c1_gt + segment_length * np.stack([np.cos(yaw_gt), np.sin(yaw_gt)], axis=-1)\n",
    "\n",
    "    # Compute distances between corresponding points\n",
    "    dist_c1 = np.linalg.norm(c1_pred - c1_gt, axis=-1)\n",
    "    dist_c2 = np.linalg.norm(c2_pred - c2_gt, axis=-1)\n",
    "\n",
    "    # Compute pose metric\n",
    "    pose_metric = np.sqrt((dist_c1 ** 2 + dist_c2 ** 2) / 2.0)\n",
    "    metric = np.mean(pose_metric)\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TrajectoryLSTM(input_size=4, hidden_size=64, num_layers=2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 94.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0365, Metric: 0.1405\n",
      "Validation Loss: 0.0327, Validation Metric: 0.1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 94.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0365, Metric: 0.1410\n",
      "Validation Loss: 0.0320, Validation Metric: 0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 93.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0361, Metric: 0.1381\n",
      "Validation Loss: 0.0314, Validation Metric: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 95.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0359, Metric: 0.1371\n",
      "Validation Loss: 0.0308, Validation Metric: 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 94.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0362, Metric: 0.1387\n",
      "Validation Loss: 0.0313, Validation Metric: 0.1249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 92.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0357, Metric: 0.1367\n",
      "Validation Loss: 0.0313, Validation Metric: 0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 93.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0355, Metric: 0.1349\n",
      "Validation Loss: 0.0325, Validation Metric: 0.1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 92.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0357, Metric: 0.1368\n",
      "Validation Loss: 0.0312, Validation Metric: 0.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 93.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0355, Metric: 0.1361\n",
      "Validation Loss: 0.0315, Validation Metric: 0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 91.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0352, Metric: 0.1346\n",
      "Validation Loss: 0.0316, Validation Metric: 0.1235\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Metric Calculation\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    for vehicle_features, initial_localization, control_sequence, target_traj in tqdm(train_loader):\n",
    "        vehicle_features = vehicle_features.to(device)\n",
    "        initial_localization = initial_localization.to(device)\n",
    "        control_sequence = control_sequence.to(device)\n",
    "        target_traj = target_traj.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(vehicle_features, initial_localization, control_sequence)\n",
    "        loss = criterion(output, target_traj)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate metric\n",
    "        output_np = output.detach().cpu().numpy()\n",
    "        target_np = target_traj.detach().cpu().numpy()\n",
    "        batch_metric = calculate_metric_on_batch(output_np, target_np)\n",
    "        epoch_metric += batch_metric\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_metric = epoch_metric / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Metric: {avg_metric:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "    with torch.no_grad():\n",
    "        for vehicle_features, initial_localization, control_sequence, target_traj in val_loader:\n",
    "            vehicle_features = vehicle_features.to(device)\n",
    "            initial_localization = initial_localization.to(device)\n",
    "            control_sequence = control_sequence.to(device)\n",
    "            target_traj = target_traj.to(device)\n",
    "\n",
    "            output = model(vehicle_features, initial_localization, control_sequence)\n",
    "            loss = criterion(output, target_traj)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate metric\n",
    "            output_np = output.detach().cpu().numpy()\n",
    "            target_np = target_traj.detach().cpu().numpy()\n",
    "            batch_metric = calculate_metric_on_batch(output_np, target_np)\n",
    "            val_metric += batch_metric\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_metric = val_metric / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Metric: {avg_val_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataset, normalizer):\n",
    "        self.data = []\n",
    "        sampling_interval_ns = 4e7  # 0.04 seconds in nanoseconds\n",
    "\n",
    "        for testcase_id, testcase in dataset.items():\n",
    "            # Get metadata\n",
    "            metadata = testcase['metadata']\n",
    "            vehicle_model = vehicle_model_encoder.transform([metadata['vehicle_model']])[0]\n",
    "            vehicle_modification = vehicle_modification_encoder.transform([metadata['vehicle_model_modification']])[0]\n",
    "            try:\n",
    "                tires_front = tires_encoder.transform([metadata['tires']['front']])[0]\n",
    "            except:\n",
    "                tires_front = tires_encoder.transform([0])[0]\n",
    "            try:\n",
    "                tires_rear = tires_encoder.transform([metadata['tires']['rear']])[0]\n",
    "            except:\n",
    "                tires_rear = tires_encoder.transform([0])[0]\n",
    "                \n",
    "                \n",
    "            vehicle_features = [vehicle_model, vehicle_modification, tires_front, tires_rear]\n",
    "\n",
    "            # Get control commands\n",
    "            control = testcase['control']\n",
    "            control['acceleration_level'] = control['acceleration_level'].fillna(0)\n",
    "            control_seq = control[['stamp_ns', 'acceleration_level', 'steering']].values\n",
    "\n",
    "            # Get initial localization data (first 5 seconds)\n",
    "            localization = testcase['localization']\n",
    "            localization_seq = localization[['stamp_ns', 'x', 'y', 'yaw']].values\n",
    "\n",
    "            # Resample to fixed time steps\n",
    "            time_steps_control = np.arange(control_seq[0, 0], control_seq[-1, 0] + sampling_interval_ns, sampling_interval_ns)\n",
    "            control_resampled = self.resample_sequence(control_seq, time_steps_control)\n",
    "\n",
    "            time_steps_loc = np.arange(localization_seq[0, 0], localization_seq[-1, 0] + sampling_interval_ns, sampling_interval_ns)\n",
    "            localization_resampled = self.resample_sequence(localization_seq, time_steps_loc)\n",
    "\n",
    "            # Prepare data\n",
    "            initial_localization = localization_resampled[:, 1:]  # Skip stamp_ns\n",
    "            # Convert yaw to sin and cos\n",
    "            yaw = initial_localization[:, 2]\n",
    "            sin_yaw = np.sin(yaw)\n",
    "            cos_yaw = np.cos(yaw)\n",
    "            initial_localization = np.hstack((initial_localization[:, :2], sin_yaw[:, np.newaxis], cos_yaw[:, np.newaxis]))  # Shape: [125, 4]\n",
    "\n",
    "            # Control sequence from 5s to 20s\n",
    "            idx_start = int(5 / 0.04)  # Starting index at 5 seconds\n",
    "            control_sequence = control_resampled[idx_start:idx_start+375, 1:]  # Shape: [375, 2]\n",
    "\n",
    "            # Normalize data\n",
    "            initial_localization[:, 0] = (initial_localization[:, 0] - normalizer['x_mean']) / normalizer['x_std']\n",
    "            initial_localization[:, 1] = (initial_localization[:, 1] - normalizer['y_mean']) / normalizer['y_std']\n",
    "            control_sequence[:, 0] = (control_sequence[:, 0] - normalizer['acceleration_mean']) / normalizer['acceleration_std']\n",
    "            control_sequence[:, 1] = (control_sequence[:, 1] - normalizer['steering_mean']) / normalizer['steering_std']\n",
    "\n",
    "            requested_stamps = testcase['requested_stamps']['stamp_ns'].values\n",
    "\n",
    "            self.data.append({\n",
    "                'testcase_id': testcase_id,\n",
    "                'vehicle_features': vehicle_features,\n",
    "                'initial_localization': initial_localization,\n",
    "                'control_sequence': control_sequence,\n",
    "                'requested_stamps': requested_stamps\n",
    "            })\n",
    "\n",
    "    def resample_sequence(self, seq, time_steps):\n",
    "        df_seq = pd.DataFrame(seq, columns=['stamp_ns'] + [f'feat_{i}' for i in range(seq.shape[1]-1)])\n",
    "        df_seq = df_seq.set_index('stamp_ns').reindex(time_steps, method='nearest').reset_index()\n",
    "        return df_seq.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        vehicle_features = torch.tensor(sample['vehicle_features'], dtype=torch.long)\n",
    "        initial_localization = torch.tensor(sample['initial_localization'], dtype=torch.float32)\n",
    "        control_sequence = torch.tensor(sample['control_sequence'], dtype=torch.float32)\n",
    "        requested_stamps = sample['requested_stamps']\n",
    "        testcase_id = sample['testcase_id']\n",
    "        return testcase_id, vehicle_features, initial_localization, control_sequence, requested_stamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:23<00:00, 339.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_data = TestDataset(test_dataset, normalizer=normalizer)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for testcase_id, vehicle_features, initial_localization, control_sequence, requested_stamps in tqdm(test_loader):\n",
    "        vehicle_features = vehicle_features.to(device)\n",
    "        initial_localization = initial_localization.to(device)\n",
    "        control_sequence = control_sequence.to(device)\n",
    "\n",
    "        output = model(vehicle_features, initial_localization, control_sequence)\n",
    "        output = output.cpu().numpy()[0]  # Shape: [375, 4]\n",
    "\n",
    "        # Denormalize positions\n",
    "        x_pred = output[:, 0] * normalizer['x_std'] + normalizer['x_mean']\n",
    "        y_pred = output[:, 1] * normalizer['y_std'] + normalizer['y_mean']\n",
    "        # Convert sin and cos back to yaw\n",
    "        yaw_pred = np.arctan2(output[:, 2], output[:, 3])\n",
    "\n",
    "        # Get indices corresponding to requested stamps\n",
    "        time_steps = np.arange(5 * 1e9, 20 * 1e9, 4e7)  # From 5s to 20s every 0.04s\n",
    "        indices = np.searchsorted(time_steps, requested_stamps - 5 * 1e9)\n",
    "\n",
    "        x_pred = x_pred[indices]\n",
    "        y_pred = y_pred[indices]\n",
    "        yaw_pred = yaw_pred[indices]\n",
    "\n",
    "        for stamp_ns, x, y, yaw in zip(requested_stamps, x_pred, y_pred, yaw_pred):\n",
    "            predictions.append({\n",
    "                'testcase_id': testcase_id.item(),\n",
    "                'stamp_ns': stamp_ns.detach().cpu().numpy().astype(int),\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'yaw': yaw\n",
    "            })\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df = submission_df[['testcase_id', 'stamp_ns', 'x', 'y', 'yaw']]\n",
    "submission_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSECS_IN_SEC = 1000000000\n",
    "\n",
    "def secs_to_nsecs(secs: float):\n",
    "    return int(secs * NSECS_IN_SEC)\n",
    "\n",
    "def nsecs_to_secs(nsecs: int):\n",
    "    return float(nsecs) / NSECS_IN_SEC\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pose prediction logic without taking into account control states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_df_to_poses(loc_df):\n",
    "    poses = []\n",
    "    for stamp_ns, x, y, yaw in zip(loc_df['stamp_ns'], loc_df['x'], loc_df['y'], loc_df['yaw']):\n",
    "        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n",
    "    return poses\n",
    "\n",
    "# naive estimation of speed at last known localization pose\n",
    "def dummy_estimate_last_speed(localization_poses):\n",
    "    last_pose = localization_poses[-1]\n",
    "    \n",
    "    start_pose_idx = -1\n",
    "    for i, pose in enumerate(localization_poses, start=1-len(localization_poses)):\n",
    "        start_pose_idx = i\n",
    "        if nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(pose['stamp_ns']) > 1.: # sec\n",
    "            break\n",
    "            \n",
    "    start_pose = localization_poses[start_pose_idx]\n",
    "    dt_sec = nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(start_pose['stamp_ns'])\n",
    "    \n",
    "    if dt_sec > 1e-5:\n",
    "        return np.linalg.norm(last_pose['pos'][:2] - start_pose['pos'][:2]) / dt_sec\n",
    "    return 5. # some default value\n",
    "\n",
    "def dummpy_predict_pose(last_loc_pose: dict, last_speed: float, prediction_stamp: int):\n",
    "    dt_sec = nsecs_to_secs(prediction_stamp) - nsecs_to_secs(last_loc_pose['stamp_ns'])\n",
    "    distance = dt_sec * last_speed\n",
    "    direction = yaw_direction(last_loc_pose['yaw'])\n",
    "    pos_translate = direction * distance\n",
    "    return {\"pos\": last_loc_pose['pos'] + pos_translate, 'yaw': last_loc_pose['yaw']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testcase(testcase: dict):\n",
    "    loc_df = testcase['localization']\n",
    "    localization_poses = localization_df_to_poses(loc_df)\n",
    "    \n",
    "    last_loc_pose = localization_poses[-1]\n",
    "    last_speed = dummy_estimate_last_speed(localization_poses)\n",
    "    \n",
    "    predicted_poses = []\n",
    "    for stamp in testcase['requested_stamps']['stamp_ns']:\n",
    "        pose = dummpy_predict_pose(last_loc_pose, last_speed, stamp)\n",
    "        predicted_poses.append(pose)\n",
    "        \n",
    "    predictions = {}\n",
    "    predictions['stamp_ns'] = testcase['requested_stamps']['stamp_ns']\n",
    "    predictions['x'] = [pose['pos'][0] for pose in predicted_poses]\n",
    "    predictions['y'] = [pose['pos'][1] for pose in predicted_poses]\n",
    "    predictions['yaw'] = [pose['yaw'] for pose in predicted_poses]\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "def predict_test_dataset(test_dataset: dict):\n",
    "    predictions = {}\n",
    "    for testcase_id, testcase in tqdm(test_dataset.items()): \n",
    "        predictions[testcase_id] = predict_testcase(testcase)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make prediction for requested stamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = predict_test_dataset(test_dataset)\n",
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(dataset_predictions: dict, prediction_file_path: str):\n",
    "    prediction_list = []\n",
    "    for testcase_id, prediction in tqdm(dataset_predictions.items()):\n",
    "        prediction['testcase_id'] = [testcase_id] * len(prediction)\n",
    "        prediction_list.append(prediction)\n",
    "    predictions_df = pd.concat(prediction_list)\n",
    "    predictions_df = predictions_df.reindex(columns=[\"testcase_id\", \"stamp_ns\", \"x\", \"y\", \"yaw\"])\n",
    "    print(len(predictions_df))\n",
    "    predictions_df.to_csv(prediction_file_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions(test_predictions, os.path.join(ROOT_DATA_FOLDER, \"predictions.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n",
    "$$\n",
    "(x_1, y_1) = (x, y), \\\\\n",
    "(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n",
    "$$  \n",
    "\n",
    "where $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n",
    "\n",
    "\n",
    "Metric for a single pose (rmse):\n",
    "\n",
    "$$\n",
    "pose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n",
    "$$\n",
    "\n",
    "where $k$ - number of points that describe single pose (in our case $k=2$).\n",
    "\n",
    "Metric for a testcase:\n",
    "\n",
    "$$\n",
    "testcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of localization points to predict.\n",
    "\n",
    "And, final metric for a whole dataset:\n",
    "\n",
    "$$\n",
    "dataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation of the metric calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEGMENT_LENGTH = 1.\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])\n",
    "\n",
    "def build_car_points(x_y_yaw):\n",
    "    directions = np.vstack(yaw_direction(x_y_yaw[:, -1]))\n",
    "    \n",
    "    front_points = x_y_yaw[:, :-1] + SEGMENT_LENGTH * directions.T\n",
    "    points = np.vstack([x_y_yaw[:, :-1], front_points])\n",
    "    return points\n",
    "\n",
    "def build_car_points_from_merged_df(df: pd.DataFrame):\n",
    "    points_gt = df[['x_gt', 'y_gt', 'yaw_gt']].to_numpy()\n",
    "    points_pred = df[['x_pred', 'y_pred', 'yaw_pred']].to_numpy()\n",
    "    \n",
    "    points_gt = build_car_points(points_gt)\n",
    "    points_pred = build_car_points(points_pred)\n",
    "    return points_gt, points_pred\n",
    "\n",
    "def calculate_metric_testcase(df: pd.DataFrame):        \n",
    "    points_gt, points_pred = build_car_points_from_merged_df(df)\n",
    "    \n",
    "    metric = np.mean(np.sqrt(2. * np.mean((points_gt - points_pred) ** 2, axis=1)))\n",
    "    return metric\n",
    "\n",
    "def calculate_metric_dataset(ground_truth_df: pd.DataFrame, prediction_df: pd.DataFrame):\n",
    "    assert (len(ground_truth_df) == len(prediction_df))\n",
    "    \n",
    "    df = ground_truth_df.merge(prediction_df, on=['testcase_id', 'stamp_ns'], suffixes=['_gt', '_pred'])\n",
    "    \n",
    "    metric = df.groupby('testcase_id').apply(calculate_metric_testcase)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
