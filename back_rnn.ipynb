{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap \n",
    "- Align stamps_ns\n",
    "- Encode points\n",
    "- Save dataset\n",
    "- Change step size, augment dataset\n",
    "- Add attention\n",
    "- Check train/val split\n",
    "- Merge predictions\n",
    "- pseudolabling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_FOLDER = \"./YandexCup2024v2\"\n",
    "\n",
    "TRAIN_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTrain\")\n",
    "TEST_DATASET_PATH = os.path.join(ROOT_DATA_FOLDER, \"YaCupTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ids of a dataset\n",
    "\n",
    "def read_testcase_ids(dataset_path: str):\n",
    "    ids = [int(case_id) for case_id in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, case_id))]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = read_testcase_ids(TRAIN_DATASET_PATH)\n",
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = read_testcase_ids(TEST_DATASET_PATH)\n",
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFilePaths:\n",
    "    def __init__(self, testcase_path: str):\n",
    "        self.testcase_path = testcase_path\n",
    "        \n",
    "    def localization(self):\n",
    "        return os.path.join(self.testcase_path, 'localization.csv')\n",
    "    \n",
    "    def control(self):\n",
    "        return os.path.join(self.testcase_path, 'control.csv')\n",
    "    \n",
    "    def metadata(self):\n",
    "        return os.path.join(self.testcase_path, 'metadata.json')\n",
    "    \n",
    "    # exists only for test_dataset\n",
    "    def requested_stamps(self):\n",
    "        return os.path.join(self.testcase_path, 'requested_stamps.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_localization(localization_path: str):\n",
    "    return pd.read_csv(localization_path)\n",
    "\n",
    "def read_control(control_path):\n",
    "    return pd.read_csv(control_path)\n",
    "\n",
    "def read_metadata(metadata_path: str):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_requested_stamps(requested_stamps_path: str):\n",
    "    return pd.read_csv(requested_stamps_path)\n",
    "    \n",
    "def read_testcase(dataset_path: str, testcase_id: str, is_test: bool = False):\n",
    "    testcase_path = os.path.join(dataset_path, str(testcase_id))\n",
    "    data_file_paths = DataFilePaths(testcase_path)\n",
    "    \n",
    "    testcase_data = {}\n",
    "    testcase_data['localization'] = read_localization(data_file_paths.localization())\n",
    "    testcase_data['control'] = read_control(data_file_paths.control())\n",
    "    testcase_data['metadata'] = read_metadata(data_file_paths.metadata())\n",
    "    if is_test:\n",
    "        testcase_data['requested_stamps'] = read_requested_stamps(data_file_paths.requested_stamps())\n",
    "        \n",
    "    return testcase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testcases(dataset_path: str, is_test: bool = False, testcase_ids: typing.Iterable[int] = None):\n",
    "    result = {}\n",
    "    if testcase_ids is None:\n",
    "        testcase_ids = read_testcase_ids(dataset_path)\n",
    "    \n",
    "    for testcase_id in tqdm(testcase_ids):\n",
    "        testcase = read_testcase(dataset_path, testcase_id, is_test=is_test)\n",
    "        result[testcase_id] = testcase\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42000/42000 [05:28<00:00, 127.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# may take some time\n",
    "train_dataset = read_testcases(TRAIN_DATASET_PATH,  testcase_ids = train_ids[:])\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:25<00:00, 309.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = read_testcases(TEST_DATASET_PATH, is_test=True)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_models = []\n",
    "vehicle_modifications = []\n",
    "tires_types = []\n",
    "location_ids = []\n",
    "vehicle_ids = []\n",
    "\n",
    "\n",
    "for testcase in train_dataset.values():\n",
    "    metadata = testcase['metadata']\n",
    "    vehicle_ids.append(metadata['vehicle_id'])\n",
    "    vehicle_models.append(metadata['vehicle_model'])\n",
    "    vehicle_modifications.append(metadata['vehicle_model_modification'])\n",
    "    location_ids.append(metadata['location_reference_point_id'])\n",
    "    tires_types.append(metadata['tires']['front'])\n",
    "    tires_types.append(metadata['tires']['rear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_vehicle_model_idx = Counter(vehicle_models).most_common()[0][0]\n",
    "unknown_vehicle_modification_idx = Counter(vehicle_modifications).most_common()[0][0]\n",
    "unknown_tires_idx = Counter(tires_types).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_ids = set(vehicle_ids)\n",
    "vehicle_models = set(vehicle_models)\n",
    "vehicle_modifications = set(vehicle_modifications)\n",
    "tires_types = set(tires_types)\n",
    "location_ids = set(location_ids)\n",
    "\n",
    "\n",
    "vehicle_model_mapping = {label: idx for idx, label in enumerate(vehicle_models)}\n",
    "vehicle_modification_mapping = {label: idx for idx, label in enumerate(vehicle_modifications)}\n",
    "tires_mapping = {label: idx for idx, label in enumerate(tires_types)}\n",
    "\n",
    "# unknown_vehicle_model_idx = len(vehicle_models)\n",
    "# unknown_vehicle_modification_idx = len(vehicle_modifications)\n",
    "# unknown_tires_idx = len(tires_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_identifiers(identifiers, val_ratio=0.2):\n",
    "    identifiers = list(identifiers)\n",
    "    random.shuffle(identifiers)\n",
    "    val_size = int(len(identifiers) * val_ratio)\n",
    "    val_identifiers = set(identifiers[:val_size])\n",
    "    train_identifiers = set(identifiers[val_size:])\n",
    "    return train_identifiers, val_identifiers\n",
    "\n",
    "# Split location_reference_point_id\n",
    "train_location_ids, val_location_ids = split_identifiers(location_ids, val_ratio=val_ratio)\n",
    "\n",
    "# Optionally, split vehicle_ids and vehicle_models similarly\n",
    "train_vehicle_ids, val_vehicle_ids = split_identifiers(vehicle_ids)\n",
    "train_vehicle_models, val_vehicle_models = split_identifiers(vehicle_modifications)\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "\n",
    "for testcase_id, testcase in train_dataset.items():\n",
    "    metadata = testcase['metadata']\n",
    "    location_id = metadata['location_reference_point_id']\n",
    "    vehicle_id = metadata['vehicle_id']\n",
    "    vehicle_model = metadata['vehicle_model']\n",
    "    \n",
    "    # Determine whether to include the sample in training or validation set\n",
    "    if (location_id in train_location_ids and\n",
    "        vehicle_id in train_vehicle_ids and\n",
    "        vehicle_model in train_vehicle_models):\n",
    "        train_data[testcase_id] = testcase\n",
    "    else:\n",
    "        val_data[testcase_id] = testcase\n",
    "\n",
    "# Check for overlaps\n",
    "assert not set(train_data.keys()) & set(val_data.keys()), \"Overlap between training and validation sets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataset, training=True):\n",
    "        self.data = []\n",
    "        sampling_interval_ns = 4e7  # 0.04 seconds in nanoseconds\n",
    "        initial_state_length = int(5 / 0.04)  # Steps for initial 5 seconds (125 steps)\n",
    "        target_length = int(15 / 0.04)  # Steps from 5s to 20s (375 steps)\n",
    "        sequence_length = initial_state_length + target_length  # Total steps (500 steps)\n",
    "\n",
    "        for testcase_id, testcase in tqdm(dataset.items()):\n",
    "            metadata = testcase['metadata']\n",
    "            vehicle_features = self.encode_vehicle_features(metadata)\n",
    "\n",
    "            # Get control commands\n",
    "            control = testcase['control']\n",
    "            control['acceleration_level'] = control['acceleration_level']\n",
    "            control_seq = control[['stamp_ns', 'acceleration_level', 'steering']].values\n",
    "\n",
    "            # Get localization data\n",
    "            localization = testcase['localization']\n",
    "            localization_seq = localization[['stamp_ns', 'x', 'y', 'z', 'roll', 'pitch', 'yaw']].values\n",
    "    \n",
    "            time_steps = np.arange(0, 60 * 1e9, sampling_interval_ns)\n",
    "            control_resampled = self.resample_sequence(control_seq, time_steps)\n",
    "            control_resampled = control_resampled[:, 1:] # drop ns\n",
    "            localization_resampled = self.resample_sequence(localization_seq, time_steps)\n",
    "            localization_resampled = localization_resampled[:, 1:] # drop ns\n",
    "            \n",
    "            max_start_idx = len(time_steps) - sequence_length\n",
    "            for i in range(0, max_start_idx, initial_state_length):  # Slide window\n",
    "                # Initial localization sequence (first 5 seconds)\n",
    "                \n",
    "                input_localization = deepcopy(localization_resampled[i:i+initial_state_length])  # Shape: [125, 3]\n",
    "                \n",
    "                start_position = deepcopy(input_localization[0][:3])\n",
    "                \n",
    "                input_localization[:, :3] -= start_position # Shift initial localization to zero\n",
    "                \n",
    "                # Target trajectory from t + 5s to t + 20s\n",
    "                output_localization = deepcopy(localization_resampled[i+initial_state_length:i+sequence_length]) # Shape: [375, 3]\n",
    "                output_localization[:, :3] -= start_position\n",
    "            \n",
    "                # Initial and inference control sequence\n",
    "                input_control_sequence = deepcopy(control_resampled[i:i+initial_state_length])\n",
    "                output_control_sequence = deepcopy(control_resampled[i+initial_state_length:i+sequence_length])\n",
    "                \n",
    "            \n",
    "                self.data.append({\n",
    "                    'vehicle_features': vehicle_features,\n",
    "                    'input_localization': input_localization,\n",
    "                    'output_localization': output_localization,\n",
    "                    'input_control_sequence': input_control_sequence,\n",
    "                    'output_control_sequence': output_control_sequence,\n",
    "                })\n",
    "\n",
    "    def encode_vehicle_features(self, metadata):\n",
    "        vehicle_model = vehicle_model_mapping.get(metadata['vehicle_model'], unknown_vehicle_model_idx)\n",
    "        vehicle_modification = vehicle_modification_mapping.get(metadata['vehicle_model_modification'], unknown_vehicle_modification_idx)\n",
    "        tires_front = tires_mapping.get(metadata['tires']['front'], unknown_tires_idx)\n",
    "        tires_rear = tires_mapping.get(metadata['tires']['rear'], unknown_tires_idx)\n",
    "        vehicle_features = [vehicle_model, vehicle_modification, tires_front, tires_rear]\n",
    "        return vehicle_features\n",
    "\n",
    "    def resample_sequence(self, seq, time_steps):\n",
    "        df_seq = pd.DataFrame(seq, columns=['stamp_ns'] + [f'feat_{i}' for i in range(seq.shape[1]-1)])\n",
    "        df_seq = df_seq.set_index('stamp_ns').reindex(time_steps, method='nearest').reset_index()\n",
    "        return df_seq.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        tensor_dict = {}\n",
    "        for k, v in sample.items():\n",
    "            if k.startswith('vehicle'):\n",
    "                tensor_dict[k] = torch.tensor(v, dtype=torch.long)\n",
    "            else:\n",
    "                tensor_dict[k] = torch.tensor(v, dtype=torch.float32)\n",
    "        return tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32508/32508 [02:36<00:00, 207.79it/s]\n",
      "100%|██████████| 9492/9492 [00:48<00:00, 194.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataset = TrajectoryDataset(dict(itertools.islice(train_data.items(), 10)), training=True)\n",
    "# val_dataset = TrajectoryDataset(dict(itertools.islice(val_data.items(), 10)), training=False)\n",
    "\n",
    "train_dataset = TrajectoryDataset(train_data, training=True)\n",
    "val_dataset = TrajectoryDataset(val_data, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_features: torch.Size([256, 4])\n",
      "input_localization: torch.Size([256, 125, 6])\n",
      "output_localization: torch.Size([256, 375, 6])\n",
      "input_control_sequence: torch.Size([256, 125, 2])\n",
      "output_control_sequence: torch.Size([256, 375, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.5273, 12.7191,  0.0213])\n",
      "tensor([18.7504, 12.8680,  0.0242])\n",
      "tensor([18.9805, 13.0272,  0.0250])\n",
      "tensor([19.2062, 13.1811,  0.0277])\n",
      "tensor([19.4358, 13.3407,  0.0221])\n",
      "tensor([19.6640, 13.4962,  0.0240])\n",
      "tensor([19.8968, 13.6516,  0.0257])\n"
     ]
    }
   ],
   "source": [
    "for ind in [-3, -2, -1]:\n",
    "    print(batch['input_localization'][batch_id][ind][:3])\n",
    "for ind in [0, 1, 2, 3]:\n",
    "    print(batch['output_localization'][batch_id][ind][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vehicle_feature_sizes, embedding_dim, localization_input_size, control_input_size, hidden_size, num_layers):\n",
    "        super(TrajectoryEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Vehicle feature embeddings\n",
    "        self.vehicle_model_embedding = nn.Embedding(num_embeddings=vehicle_feature_sizes['vehicle_model'], embedding_dim=embedding_dim)\n",
    "        self.vehicle_modification_embedding = nn.Embedding(num_embeddings=vehicle_feature_sizes['vehicle_modification'], embedding_dim=embedding_dim)\n",
    "        self.tires_embedding = nn.Embedding(num_embeddings=vehicle_feature_sizes['tires'], embedding_dim=embedding_dim)\n",
    "\n",
    "        # Fully connected layer to combine vehicle features\n",
    "        self.vehicle_fc = nn.Linear(embedding_dim * 4, hidden_size)\n",
    "\n",
    "        # Encoder LSTM for localization\n",
    "        self.localization_encoder = nn.LSTM(input_size=localization_input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Encoder LSTM for control sequence\n",
    "        self.control_encoder = nn.LSTM(input_size=control_input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Decoder LSTM\n",
    "        self.decoder = nn.LSTM(input_size=control_input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, localization_input_size)\n",
    "\n",
    "    def forward(self, vehicle_features, input_localization, input_control_sequence, output_control_sequence):\n",
    "        batch_size = input_localization.size(0)\n",
    "\n",
    "        # Embed vehicle features\n",
    "        vehicle_model = self.vehicle_model_embedding(vehicle_features[:, 0])\n",
    "        vehicle_modification = self.vehicle_modification_embedding(vehicle_features[:, 1])\n",
    "        tires_front = self.tires_embedding(vehicle_features[:, 2])\n",
    "        tires_rear = self.tires_embedding(vehicle_features[:, 3])\n",
    "\n",
    "        # Concatenate vehicle features\n",
    "        vehicle_embedded = torch.cat([vehicle_model, vehicle_modification, tires_front, tires_rear], dim=1)\n",
    "        vehicle_features_encoded = self.vehicle_fc(vehicle_embedded)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Encoder for localization\n",
    "        _, (hidden_loc, cell_loc) = self.localization_encoder(input_localization)  # hidden_loc: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Encoder for control sequence\n",
    "        _, (hidden_ctrl, cell_ctrl) = self.control_encoder(input_control_sequence)  # hidden_ctrl: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Combine encoder hidden states and vehicle features\n",
    "        # Option to concatenate, sum, or average hidden states\n",
    "        hidden_enc = (hidden_loc + hidden_ctrl) / 2  # Shape: [num_layers, batch_size, hidden_size]\n",
    "        cell_enc = (cell_loc + cell_ctrl) / 2\n",
    "\n",
    "        # Incorporate vehicle features into the hidden state\n",
    "        # We'll add vehicle_features_encoded to the first layer's hidden state\n",
    "        hidden_enc[0] = hidden_enc[0] + vehicle_features_encoded.unsqueeze(0)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_output, _ = self.decoder(output_control_sequence, (hidden_enc, cell_enc))  # decoder_output: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Output layer\n",
    "        output_localization = self.fc_out(decoder_output)  # Shape: [batch_size, seq_len, localization_input_size]\n",
    "\n",
    "        return output_localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes based on your data\n",
    "vehicle_feature_sizes = {\n",
    "    'vehicle_model': len(vehicle_model_mapping),\n",
    "    'vehicle_modification': len(vehicle_modification_mapping),\n",
    "    'tires': len(tires_mapping),\n",
    "}\n",
    "\n",
    "embedding_dim = 16\n",
    "localization_input_size = 6  # For example, x, y, z, roll, pitch, yaw\n",
    "control_input_size = 2  # acceleration_level, steering\n",
    "hidden_size = 256\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEGMENT_LENGTH = 1.0\n",
    "\n",
    "def calculate_metric_on_batch(output_np, target_np, segment_length=1.0):\n",
    "    \"\"\"\n",
    "    output_np: numpy array of shape [batch_size, seq_len, 4], predicted x, y, yaw\n",
    "    target_np: numpy array of same shape, ground truth x, y, yaw\n",
    "\n",
    "    Returns:\n",
    "        metric: float, the average metric over the batch\n",
    "    \"\"\"\n",
    "    x_pred, y_pred, yaw_pred = output_np[..., 0], output_np[..., 1], output_np[..., 2]\n",
    "    x_gt, y_gt, yaw_gt = target_np[..., 0], target_np[..., 1], target_np[..., 2]\n",
    "\n",
    "    # Compute c1 and c2 for predicted\n",
    "    c1_pred = np.stack([x_pred, y_pred], axis=-1)\n",
    "    c2_pred = c1_pred + segment_length * np.stack([np.cos(yaw_pred), np.sin(yaw_pred)], axis=-1)\n",
    "\n",
    "    # Compute c1 and c2 for ground truth\n",
    "    c1_gt = np.stack([x_gt, y_gt], axis=-1)\n",
    "    c2_gt = c1_gt + segment_length * np.stack([np.cos(yaw_gt), np.sin(yaw_gt)], axis=-1)\n",
    "\n",
    "    # Compute distances between corresponding points\n",
    "    dist_c1 = np.linalg.norm(c1_pred - c1_gt, axis=-1)\n",
    "    dist_c2 = np.linalg.norm(c2_pred - c2_gt, axis=-1)\n",
    "\n",
    "    # Compute pose metric\n",
    "    pose_metric = np.sqrt((dist_c1 ** 2 + dist_c2 ** 2) / 2.0)\n",
    "    metric = np.mean(pose_metric)\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TrajectoryEncoderDecoder(\n",
    "    vehicle_feature_sizes=vehicle_feature_sizes,\n",
    "    embedding_dim=embedding_dim,\n",
    "    localization_input_size=localization_input_size,\n",
    "    control_input_size=control_input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 677.6490, Metric: 42.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 268.5873, Metric: 25.6651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Training Loss: 162.7855, Metric: 18.5048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 105.5742, Metric: 15.7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Training Loss: 77.3995, Metric: 12.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 54.3825, Metric: 10.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Training Loss: 51.4208, Metric: 10.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 37.4639, Metric: 9.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:16<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Training Loss: 35.2056, Metric: 8.8369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 27.4628, Metric: 8.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Training Loss: 23.2964, Metric: 7.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 19.4662, Metric: 6.6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Training Loss: 19.7027, Metric: 6.7783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:26<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 16.3331, Metric: 6.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Training Loss: 14.6562, Metric: 5.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 12.6309, Metric: 5.5191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Training Loss: 11.3420, Metric: 5.2904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.5695, Metric: 4.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Training Loss: 10.0438, Metric: 5.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.2180, Metric: 4.7895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Training Loss: 8.2181, Metric: 4.5898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.8271, Metric: 4.4556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Training Loss: 7.7790, Metric: 4.4702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:26<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.2394, Metric: 4.5411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Training Loss: 7.5433, Metric: 4.3582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.1294, Metric: 4.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Training Loss: 6.2082, Metric: 4.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.3567, Metric: 4.6710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:16<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Training Loss: 6.0269, Metric: 3.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.9994, Metric: 3.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Training Loss: 5.4768, Metric: 3.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.0471, Metric: 4.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Training Loss: 5.3797, Metric: 3.7279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.7546, Metric: 3.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:16<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Training Loss: 5.4258, Metric: 3.7421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.3778, Metric: 3.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Training Loss: 5.4513, Metric: 3.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.0034, Metric: 3.6070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Training Loss: 4.5918, Metric: 3.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:26<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.5882, Metric: 3.3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Training Loss: 4.6987, Metric: 3.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.5234, Metric: 3.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Training Loss: 4.3290, Metric: 3.3460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.1357, Metric: 3.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Training Loss: 4.1865, Metric: 3.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.5403, Metric: 3.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Training Loss: 4.0481, Metric: 3.2509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.8244, Metric: 3.5575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Training Loss: 3.9784, Metric: 3.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.1899, Metric: 3.2516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Training Loss: 3.7970, Metric: 3.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.0748, Metric: 3.2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Training Loss: 3.7623, Metric: 3.1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.8469, Metric: 3.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Training Loss: 3.5112, Metric: 3.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.7614, Metric: 3.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Training Loss: 3.4123, Metric: 2.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.9670, Metric: 3.1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Training Loss: 3.5055, Metric: 3.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.2850, Metric: 3.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Training Loss: 3.2896, Metric: 2.9355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5568, Metric: 2.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:16<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Training Loss: 3.5542, Metric: 3.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.3757, Metric: 3.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Training Loss: 3.2340, Metric: 2.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.6851, Metric: 3.0285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Training Loss: 3.0929, Metric: 2.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.9226, Metric: 3.1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Training Loss: 3.0463, Metric: 2.8310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:26<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3368, Metric: 2.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Training Loss: 3.0286, Metric: 2.8183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.5162, Metric: 3.3397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Training Loss: 3.1670, Metric: 2.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4263, Metric: 2.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Training Loss: 2.7442, Metric: 2.6881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3121, Metric: 2.8266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:14<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Training Loss: 2.7378, Metric: 2.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3853, Metric: 2.9009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Training Loss: 2.7448, Metric: 2.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3308, Metric: 2.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Training Loss: 2.9521, Metric: 2.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5846, Metric: 2.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Training Loss: 2.7610, Metric: 2.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.7561, Metric: 4.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Training Loss: 2.7837, Metric: 2.7098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.2388, Metric: 2.8046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Training Loss: 2.6856, Metric: 2.6486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5686, Metric: 2.9078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Training Loss: 2.6348, Metric: 2.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.9278, Metric: 2.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Training Loss: 2.5008, Metric: 2.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.9523, Metric: 2.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:16<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Training Loss: 3.2312, Metric: 2.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3255, Metric: 2.9142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Training Loss: 2.4398, Metric: 2.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7058, Metric: 2.5147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Training Loss: 2.3005, Metric: 2.4650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7289, Metric: 2.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1016/1016 [03:17<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Training Loss: 2.4746, Metric: 2.5456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7698, Metric: 2.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    for sample in tqdm(train_loader):\n",
    "        vehicle_features = sample['vehicle_features'].to(device)\n",
    "        input_localization = sample['input_localization'].to(device)\n",
    "        output_localization = sample['output_localization'].to(device)\n",
    "        input_control_sequence = sample['input_control_sequence'].to(device)\n",
    "        output_control_sequence = sample['output_control_sequence'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output_localization = model(\n",
    "            vehicle_features,\n",
    "            input_localization,\n",
    "            input_control_sequence,\n",
    "            output_control_sequence\n",
    "        )\n",
    "        loss = criterion(predicted_output_localization, output_localization)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        predicted_x_y_yaw = predicted_output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "        gt_x_y_yaw = output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "        batch_metric = calculate_metric_on_batch(predicted_x_y_yaw, gt_x_y_yaw)\n",
    "        epoch_metric += batch_metric\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_metric = epoch_metric / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Metric: {avg_metric:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(val_loader):\n",
    "            vehicle_features = sample['vehicle_features'].to(device)\n",
    "            input_localization = sample['input_localization'].to(device)\n",
    "            output_localization = sample['output_localization'].to(device)\n",
    "            input_control_sequence = sample['input_control_sequence'].to(device)\n",
    "            output_control_sequence = sample['output_control_sequence'].to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            predicted_output_localization = model(\n",
    "                vehicle_features,\n",
    "                input_localization,\n",
    "                input_control_sequence,\n",
    "                output_control_sequence\n",
    "            )\n",
    "            loss = criterion(predicted_output_localization, output_localization)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_x_y_yaw = predicted_output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "            gt_x_y_yaw = output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "            batch_metric = calculate_metric_on_batch(predicted_x_y_yaw, gt_x_y_yaw)\n",
    "            val_metric += batch_metric\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.jit.save(torch.jit.script(model), 'best.pt')\n",
    "        \n",
    "    avg_val_metric = val_metric / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Metric: {avg_val_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:24<00:00, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7698, Metric: 2.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "val_metric = 0\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(val_loader):\n",
    "        vehicle_features = sample['vehicle_features'].to(device)\n",
    "        input_localization = sample['input_localization'].to(device)\n",
    "        output_localization = sample['output_localization'].to(device)\n",
    "        input_control_sequence = sample['input_control_sequence'].to(device)\n",
    "        output_control_sequence = sample['output_control_sequence'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_output_localization = model(\n",
    "            vehicle_features,\n",
    "            input_localization,\n",
    "            input_control_sequence,\n",
    "            output_control_sequence\n",
    "        )\n",
    "        loss = criterion(predicted_output_localization, output_localization)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        predicted_x_y_yaw = predicted_output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "        gt_x_y_yaw = output_localization[..., [0,1,-1]].detach().cpu().numpy()\n",
    "        batch_metric = calculate_metric_on_batch(predicted_x_y_yaw, gt_x_y_yaw)\n",
    "        val_metric += batch_metric\n",
    "        \n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    torch.jit.save(torch.jit.script(model), 'best.pt')\n",
    "    \n",
    "avg_val_metric = val_metric / len(val_loader)\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}, Metric: {avg_val_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.data = []\n",
    "        sampling_interval_ns = 4e7  # 0.04 seconds in nanoseconds\n",
    "        initial_state_length = int(5 / 0.04)  # 125 steps\n",
    "        target_length = int(15 / 0.04)        # 375 steps\n",
    "        sequence_length = initial_state_length + target_length  # 500 steps\n",
    "\n",
    "        # Get list of test case IDs (folder names)\n",
    "        testcase_ids = sorted([name for name in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, name))])\n",
    "\n",
    "        for testcase_id in tqdm(testcase_ids):\n",
    "            testcase_path = os.path.join(dataset_path, testcase_id)\n",
    "\n",
    "            # Load metadata and encode vehicle features\n",
    "            with open(os.path.join(testcase_path, 'metadata.json'), 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            vehicle_features = self.encode_vehicle_features(metadata)\n",
    "\n",
    "            # Load localization data (first 5 seconds)\n",
    "            localization = pd.read_csv(os.path.join(testcase_path, 'localization.csv'))\n",
    "            localization_seq = localization[['stamp_ns', 'x', 'y', 'z', 'roll', 'pitch', 'yaw']].values\n",
    "\n",
    "            # Load control data (first 20 seconds)\n",
    "            control = pd.read_csv(os.path.join(testcase_path, 'control.csv'))\n",
    "            control['acceleration_level'] = control['acceleration_level'].fillna(0)\n",
    "            control_seq = control[['stamp_ns', 'acceleration_level', 'steering']].values\n",
    "\n",
    "            # Load requested stamps\n",
    "            requested_stamps = pd.read_csv(os.path.join(testcase_path, 'requested_stamps.csv'))['stamp_ns'].values\n",
    "\n",
    "            # Resample sequences to fixed time steps\n",
    "            time_steps_localization = np.arange(0, 5 * 1e9, sampling_interval_ns)\n",
    "            time_steps_control = np.arange(0, 20 * 1e9, sampling_interval_ns)\n",
    "\n",
    "            localization_resampled = self.resample_sequence(localization_seq, time_steps_localization)\n",
    "            control_resampled = self.resample_sequence(control_seq, time_steps_control)\n",
    "\n",
    "            # Process localization data\n",
    "            localization_resampled = localization_resampled[:, 1:]  # Drop stamp_ns\n",
    "            input_localization = localization_resampled.copy()      # Shape: [125, 7]\n",
    "\n",
    "            # Subtract start position\n",
    "            start_position = input_localization[0, :3].copy()\n",
    "            input_localization[:, :3] -= start_position\n",
    "\n",
    "            # Prepare input_control_sequence (first 5 seconds)\n",
    "            control_resampled = control_resampled[:, 1:]  # Drop stamp_ns\n",
    "            input_control_sequence = control_resampled[:initial_state_length].copy()  # [125, 2]\n",
    "\n",
    "            # Prepare output_control_sequence (from 5s to 20s)\n",
    "            output_control_sequence = control_resampled[initial_state_length:].copy()  # [375, 2]\n",
    "\n",
    "            self.data.append({\n",
    "                'testcase_id': int(testcase_id),\n",
    "                'vehicle_features': vehicle_features,\n",
    "                'input_localization': input_localization,\n",
    "                'input_control_sequence': input_control_sequence,\n",
    "                'output_control_sequence': output_control_sequence,\n",
    "                'start_position': start_position,\n",
    "                'requested_stamps': requested_stamps\n",
    "            })\n",
    "\n",
    "    def encode_vehicle_features(self, metadata):\n",
    "        vehicle_model = vehicle_model_mapping.get(metadata['vehicle_model'], unknown_vehicle_model_idx)\n",
    "        vehicle_modification = vehicle_modification_mapping.get(metadata['vehicle_model_modification'], unknown_vehicle_modification_idx)\n",
    "        tires_front = tires_mapping.get(metadata['tires']['front'], unknown_tires_idx)\n",
    "        tires_rear = tires_mapping.get(metadata['tires']['rear'], unknown_tires_idx)\n",
    "        vehicle_features = [vehicle_model, vehicle_modification, tires_front, tires_rear]\n",
    "        return vehicle_features\n",
    "\n",
    "    def resample_sequence(self, seq, time_steps):\n",
    "        df_seq = pd.DataFrame(seq, columns=['stamp_ns'] + [f'feat_{i}' for i in range(seq.shape[1]-1)])\n",
    "        df_seq = df_seq.set_index('stamp_ns').reindex(time_steps, method='nearest').reset_index()\n",
    "        return df_seq.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        tensor_dict = {}\n",
    "        for k, v in sample.items():\n",
    "            if k == 'vehicle_features':\n",
    "                tensor_dict[k] = torch.tensor(v, dtype=torch.long)\n",
    "            elif k in ['input_localization', 'input_control_sequence', 'output_control_sequence']:\n",
    "                tensor_dict[k] = torch.tensor(v, dtype=torch.float32)\n",
    "            else:\n",
    "                tensor_dict[k] = v  # Keep as is (e.g., start_position, requested_stamps, testcase_id)\n",
    "        return tensor_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:59<00:00, 133.49it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(TEST_DATASET_PATH)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_length = 125\n",
    "target_length = 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [01:41<00:00, 78.85it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(test_loader):\n",
    "        testcase_id = sample['testcase_id'][0].item()\n",
    "        vehicle_features = sample['vehicle_features'].to(device)\n",
    "        input_localization = sample['input_localization'].to(device)\n",
    "        input_control_sequence = sample['input_control_sequence'].to(device)\n",
    "        output_control_sequence = sample['output_control_sequence'].to(device)\n",
    "        start_position = sample['start_position'][0].numpy()\n",
    "        requested_stamps = sample['requested_stamps'][0].numpy()\n",
    "\n",
    "        # Check sequence lengths\n",
    "        if input_localization.size(1) != initial_state_length:\n",
    "            print(f\"Skipping {testcase_id}: input_localization length mismatch\")\n",
    "            continue\n",
    "        if input_control_sequence.size(1) != initial_state_length:\n",
    "            print(f\"Skipping {testcase_id}: input_control_sequence length mismatch\")\n",
    "            continue\n",
    "        if output_control_sequence.size(1) != target_length:\n",
    "            print(f\"Skipping {testcase_id}: output_control_sequence length mismatch\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            predicted_output_localization = model(\n",
    "                vehicle_features,\n",
    "                input_localization,\n",
    "                input_control_sequence,\n",
    "                output_control_sequence\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {testcase_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        predicted_output_localization = predicted_output_localization.detach().cpu().numpy()[0]\n",
    "        predicted_output_localization[:, :3] += start_position\n",
    "        \n",
    "        # Get x, y positions, yaw\n",
    "        yaw_pred = predicted_output_localization[:, -1]\n",
    "        x_pred = predicted_output_localization[:, 0]\n",
    "        y_pred = predicted_output_localization[:, 1]\n",
    "\n",
    "        # Time steps corresponding to output predictions (from 5s to 20s every 0.04s)\n",
    "        time_steps = np.arange(5 * 1e9, 20 * 1e9, 4e7)  # [375]\n",
    "\n",
    "        # Map requested_stamps to indices in time_steps\n",
    "        indices = np.searchsorted(time_steps, requested_stamps)\n",
    "        \n",
    "        # Handle any indices out of bounds\n",
    "        indices = np.clip(indices, 0, len(time_steps) - 1)\n",
    "\n",
    "        # Extract predictions at requested timestamps\n",
    "        x_pred = x_pred[indices]\n",
    "        y_pred = y_pred[indices]\n",
    "        yaw_pred = yaw_pred[indices]\n",
    "\n",
    "        # Collect predictions\n",
    "        for stamp_ns, x, y, yaw in zip(requested_stamps, x_pred, y_pred, yaw_pred):\n",
    "            predictions.append({\n",
    "                'testcase_id': testcase_id,\n",
    "                'stamp_ns': int(stamp_ns),\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'yaw': yaw\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testcase_id    False\n",
       "stamp_ns       False\n",
       "x              False\n",
       "y              False\n",
       "yaw            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testcase_id</th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5000888836</td>\n",
       "      <td>-1493.368408</td>\n",
       "      <td>-1308.514038</td>\n",
       "      <td>1.816736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5040043013</td>\n",
       "      <td>-1490.147339</td>\n",
       "      <td>-1311.543823</td>\n",
       "      <td>1.871183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5079989560</td>\n",
       "      <td>-1490.147339</td>\n",
       "      <td>-1311.543823</td>\n",
       "      <td>1.871183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5120797471</td>\n",
       "      <td>-1492.237915</td>\n",
       "      <td>-1310.847046</td>\n",
       "      <td>1.785788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5165218288</td>\n",
       "      <td>-1491.014771</td>\n",
       "      <td>-1311.995972</td>\n",
       "      <td>1.787042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   testcase_id    stamp_ns            x            y       yaw\n",
       "0            0  5000888836 -1493.368408 -1308.514038  1.816736\n",
       "1            0  5040043013 -1490.147339 -1311.543823  1.871183\n",
       "2            0  5079989560 -1490.147339 -1311.543823  1.871183\n",
       "3            0  5120797471 -1492.237915 -1310.847046  1.785788\n",
       "4            0  5165218288 -1491.014771 -1311.995972  1.787042"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('submissions/lstm_256.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 5)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.explode(column=['stamp_ns', 'x', 'y', 'yaw']).to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSECS_IN_SEC = 1000000000\n",
    "\n",
    "def secs_to_nsecs(secs: float):\n",
    "    return int(secs * NSECS_IN_SEC)\n",
    "\n",
    "def nsecs_to_secs(nsecs: int):\n",
    "    return float(nsecs) / NSECS_IN_SEC\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple pose prediction logic without taking into account control states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_df_to_poses(loc_df):\n",
    "    poses = []\n",
    "    for stamp_ns, x, y, yaw in zip(loc_df['stamp_ns'], loc_df['x'], loc_df['y'], loc_df['yaw']):\n",
    "        poses.append({'stamp_ns': stamp_ns, 'pos': np.array([x, y]), 'yaw': yaw})\n",
    "    return poses\n",
    "\n",
    "# naive estimation of speed at last known localization pose\n",
    "def dummy_estimate_last_speed(localization_poses):\n",
    "    last_pose = localization_poses[-1]\n",
    "    \n",
    "    start_pose_idx = -1\n",
    "    for i, pose in enumerate(localization_poses, start=1-len(localization_poses)):\n",
    "        start_pose_idx = i\n",
    "        if nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(pose['stamp_ns']) > 1.: # sec\n",
    "            break\n",
    "            \n",
    "    start_pose = localization_poses[start_pose_idx]\n",
    "    dt_sec = nsecs_to_secs(last_pose['stamp_ns']) - nsecs_to_secs(start_pose['stamp_ns'])\n",
    "    \n",
    "    if dt_sec > 1e-5:\n",
    "        return np.linalg.norm(last_pose['pos'][:2] - start_pose['pos'][:2]) / dt_sec\n",
    "    return 5. # some default value\n",
    "\n",
    "def dummpy_predict_pose(last_loc_pose: dict, last_speed: float, prediction_stamp: int):\n",
    "    dt_sec = nsecs_to_secs(prediction_stamp) - nsecs_to_secs(last_loc_pose['stamp_ns'])\n",
    "    distance = dt_sec * last_speed\n",
    "    direction = yaw_direction(last_loc_pose['yaw'])\n",
    "    pos_translate = direction * distance\n",
    "    return {\"pos\": last_loc_pose['pos'] + pos_translate, 'yaw': last_loc_pose['yaw']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testcase(testcase: dict):\n",
    "    loc_df = testcase['localization']\n",
    "    localization_poses = localization_df_to_poses(loc_df)\n",
    "    \n",
    "    last_loc_pose = localization_poses[-1]\n",
    "    last_speed = dummy_estimate_last_speed(localization_poses)\n",
    "    \n",
    "    predicted_poses = []\n",
    "    for stamp in testcase['requested_stamps']['stamp_ns']:\n",
    "        pose = dummpy_predict_pose(last_loc_pose, last_speed, stamp)\n",
    "        predicted_poses.append(pose)\n",
    "        \n",
    "    predictions = {}\n",
    "    predictions['stamp_ns'] = testcase['requested_stamps']['stamp_ns']\n",
    "    predictions['x'] = [pose['pos'][0] for pose in predicted_poses]\n",
    "    predictions['y'] = [pose['pos'][1] for pose in predicted_poses]\n",
    "    predictions['yaw'] = [pose['yaw'] for pose in predicted_poses]\n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "def predict_test_dataset(test_dataset: dict):\n",
    "    predictions = {}\n",
    "    for testcase_id, testcase in tqdm(test_dataset.items()): \n",
    "        predictions[testcase_id] = predict_testcase(testcase)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make prediction for requested stamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:23<00:00, 343.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = predict_test_dataset(test_dataset)\n",
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(dataset_predictions: dict, prediction_file_path: str):\n",
    "    prediction_list = []\n",
    "    for testcase_id, prediction in tqdm(dataset_predictions.items()):\n",
    "        prediction['testcase_id'] = [testcase_id] * len(prediction)\n",
    "        prediction_list.append(prediction)\n",
    "    predictions_df = pd.concat(prediction_list)\n",
    "    predictions_df = predictions_df.reindex(columns=[\"testcase_id\", \"stamp_ns\", \"x\", \"y\", \"yaw\"])\n",
    "    print(len(predictions_df))\n",
    "    predictions_df.to_csv(prediction_file_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 4018.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2998763\n"
     ]
    }
   ],
   "source": [
    "write_predictions(test_predictions, os.path.join(ROOT_DATA_FOLDER, \"dummy_prediction.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./YandexCup2024v2'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_prediction = pd.read_csv('./YandexCup2024v2/dummy_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testcase_id</th>\n",
       "      <th>stamp_ns</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5000888836</td>\n",
       "      <td>-1490.905035</td>\n",
       "      <td>-1310.813635</td>\n",
       "      <td>2.047693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5040043013</td>\n",
       "      <td>-1490.955001</td>\n",
       "      <td>-1310.716927</td>\n",
       "      <td>2.047693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5079989560</td>\n",
       "      <td>-1491.005979</td>\n",
       "      <td>-1310.618261</td>\n",
       "      <td>2.047693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5120797471</td>\n",
       "      <td>-1491.058057</td>\n",
       "      <td>-1310.517468</td>\n",
       "      <td>2.047693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5165218288</td>\n",
       "      <td>-1491.114744</td>\n",
       "      <td>-1310.407751</td>\n",
       "      <td>2.047693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   testcase_id    stamp_ns            x            y       yaw\n",
       "0            0  5000888836 -1490.905035 -1310.813635  2.047693\n",
       "1            0  5040043013 -1490.955001 -1310.716927  2.047693\n",
       "2            0  5079989560 -1491.005979 -1310.618261  2.047693\n",
       "3            0  5120797471 -1491.058057 -1310.517468  2.047693\n",
       "4            0  5165218288 -1491.114744 -1310.407751  2.047693"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2998763, 5)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_prediction.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe final metric. As a first step, all predicted triples $(x,y,yaw)$ are being converted into 2 points $[(x_1, y_1), (x_2, y_2)]$ in the following way:\n",
    "$$\n",
    "(x_1, y_1) = (x, y), \\\\\n",
    "(x_2, y_2) = (x_1, y_1) + S \\times (yaw_x, yaw_y)\n",
    "$$  \n",
    "\n",
    "where $S = 1$. In other words, we build a directed segment of length $1$. These points then used in the metric calculation.\n",
    "\n",
    "\n",
    "Metric for a single pose (rmse):\n",
    "\n",
    "$$\n",
    "pose\\_metric = \\sqrt{ \\frac{\\displaystyle\\sum_{j=1}^{k} {(x_j-\\hat{x_j})^2 + (y_j-\\hat{y_j})^2}}{k} }\n",
    "$$\n",
    "\n",
    "where $k$ - number of points that describe single pose (in our case $k=2$).\n",
    "\n",
    "Metric for a testcase:\n",
    "\n",
    "$$\n",
    "testcase\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}pose\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of localization points to predict.\n",
    "\n",
    "And, final metric for a whole dataset:\n",
    "\n",
    "$$\n",
    "dataset\\_metric = \\frac{1}{n}  \\displaystyle\\sum_{i=1}^{n}testcase\\_metric_i\n",
    "$$\n",
    "\n",
    "where $n$ - number of test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation of the metric calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEGMENT_LENGTH = 1.\n",
    "\n",
    "def yaw_direction(yaw_value):\n",
    "    return np.array([np.cos(yaw_value), np.sin(yaw_value)])\n",
    "\n",
    "def build_car_points(x_y_yaw):\n",
    "    directions = np.vstack(yaw_direction(x_y_yaw[:, -1]))\n",
    "    \n",
    "    front_points = x_y_yaw[:, :-1] + SEGMENT_LENGTH * directions.T\n",
    "    points = np.vstack([x_y_yaw[:, :-1], front_points])\n",
    "    return points\n",
    "\n",
    "def build_car_points_from_merged_df(df: pd.DataFrame):\n",
    "    points_gt = df[['x_gt', 'y_gt', 'yaw_gt']].to_numpy()\n",
    "    points_pred = df[['x_pred', 'y_pred', 'yaw_pred']].to_numpy()\n",
    "    \n",
    "    points_gt = build_car_points(points_gt)\n",
    "    points_pred = build_car_points(points_pred)\n",
    "    return points_gt, points_pred\n",
    "\n",
    "def calculate_metric_testcase(df: pd.DataFrame):        \n",
    "    points_gt, points_pred = build_car_points_from_merged_df(df)\n",
    "    \n",
    "    metric = np.mean(np.sqrt(2. * np.mean((points_gt - points_pred) ** 2, axis=1)))\n",
    "    return metric\n",
    "\n",
    "def calculate_metric_dataset(ground_truth_df: pd.DataFrame, prediction_df: pd.DataFrame):\n",
    "    assert (len(ground_truth_df) == len(prediction_df))\n",
    "    \n",
    "    df = ground_truth_df.merge(prediction_df, on=['testcase_id', 'stamp_ns'], suffixes=['_gt', '_pred'])\n",
    "    \n",
    "    metric = df.groupby('testcase_id').apply(calculate_metric_testcase)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
